# -*- coding: utf-8 -*-
"""Reearch Methodology Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EQJoCAvcxjpZWJtFpq-1XRJcBsYV9fL1
"""

# TensorFlow and tf.keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Dropout
import random
# Helper libraries
import numpy as np
import time
import pymongo

fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
train_images = train_images / 255.0

test_images = test_images / 255.0

train_images = train_images.reshape(60000, 784)
test_images = test_images.reshape(10000, 784)

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

mongoClient = pymongo.MongoClient("mongodb://localhost:27017/")
mongo_db = mongoClient["ml_app"]

collection = mongo_db["process_status"]


def Convert(lst):
    res_dct = {lst[i]: lst[i + 1] for i in range(0, len(lst), 2)}
    return res_dct


class GeneticAlgorithm():
    def __init__(self, population_size=20, enableDropout=False, debug=False, userEmailId='manpreetignite@gmail.com'):
        self.input_size = 784
        self.output_size = 10
        self.activation_functions = ["relu", "sigmoid", "tanh"]
        if population_size < 4:
            print("Minimum population size 4")
        if population_size % 2 != 0:
            self.sol_per_pop += 1
        else:
            self.sol_per_pop = population_size
        self.userEmailId = userEmailId
        self.maxNumberOfLayers = 5
        self.maxHiddenNodes = 5
        self.enableDropout = enableDropout
        self.population = []
        self.num_parents_mating = int(self.sol_per_pop / 2)
        if self.num_parents_mating % 2 != 0:
            self.num_parents_mating -= 1
        self.num_generations = 5
        self.mutation_percent = 20
        self.epochs_per_sol = 5
        self.debug = debug
        self.training_logs = {}
        self.initialisePopulation()
        self.new_population = []
        self.save_progress = []

    def getRandomActivationFunction(self):
        chooseActivation = random.randint(0, len(self.activation_functions) - 1)
        return self.activation_functions[chooseActivation]

    def initialisePopulation(self):
        self.activation_function_list = {}
        for i in range(self.sol_per_pop):
            self.activation_function_list[i] = []
            model = Sequential()
            model.add(Dense(random.randint(2, self.maxHiddenNodes), input_dim=self.input_size, activation='relu'))
            self.activation_function_list[i] = ["relu"]
            for j in range(random.randint(1, self.maxNumberOfLayers)):
                activation_function = self.getRandomActivationFunction()
                self.activation_function_list[i].append(activation_function)
                model.add(Dense(random.randint(2, self.maxHiddenNodes), activation=activation_function))
            if self.enableDropout:
                model.add(Dropout(random.randint(1, 5) / 10))
            model.add(Dense(self.output_size, activation="sigmoid"))
            self.activation_function_list[i].append("sigmoid")
            if self.debug == True:
                print(f"-------------Model {i + 1}----------------")
                print(model.summary())
                print(self.activation_function_list[i])
            self.population.append(model)
        if self.debug == True:
            print(self.population)

    # Converting each solution from matrix to vector.
    def mat_to_vector(self, mat_pop_weights):
        pop_weights_vector = []
        for sol_idx in range(mat_pop_weights.shape[0]):
            curr_vector = []
            for layer_idx in range(mat_pop_weights.shape[1]):
                vector_weights = np.reshape(mat_pop_weights[sol_idx, layer_idx],
                                            newshape=(mat_pop_weights[sol_idx, layer_idx].size))
                curr_vector.extend(vector_weights)
            pop_weights_vector.append(curr_vector)
        return np.array(pop_weights_vector)

    # Converting each solution from vector to matrix.
    def vector_to_mat(self, vector_pop_weights, mat_pop_weights):
        mat_weights = []
        for sol_idx in range(mat_pop_weights.shape[0]):
            start = 0
            end = 0
            for layer_idx in range(mat_pop_weights.shape[1]):
                end = end + mat_pop_weights[sol_idx, layer_idx].size
                curr_vector = vector_pop_weights[sol_idx, start:end]
                mat_layer_weights = np.reshape(curr_vector, newshape=(mat_pop_weights[sol_idx, layer_idx].shape))
                mat_weights.append(mat_layer_weights)
                start = end
        return np.reshape(mat_weights, newshape=mat_pop_weights.shape)

    def mutation(self, offspring_crossover, mutation_percent):
        num_mutations = np.uint32((mutation_percent * offspring_crossover.shape[1]) / 100)
        mutation_indices = np.array(random.sample(range(0, offspring_crossover.shape[1]), num_mutations))
        # Mutation changes a single gene in each offspring randomly.
        for idx in range(offspring_crossover.shape[0]):
            # The random value to be added to the gene.
            random_value = np.random.uniform(-1.0, 1.0, 1)
            offspring_crossover[idx, mutation_indices] = offspring_crossover[idx, mutation_indices] + random_value
        return offspring_crossover

    def reorganise_parameters(self, p1, p2, u_id):
        parent1 = p1
        parent2 = p2
        unit_id = "layer_" + str(u_id) + "_unit"
        unit_id_next = "layer_" + str(u_id + 1) + "_unit"
        weight_id = "layer_" + str(u_id + 1) + "_weight"
        bias_id = "layer_" + str(u_id) + "_bias"
        offspring_n_unit = min(parent1["units"][unit_id], parent2["units"][unit_id])
        # print(f"-------------Before----------------")
        # print(np.array(parent1["weights"][weight_id]).shape)
        # print(np.array(parent2["weights"][weight_id]).shape)
        temp = []
        temp = parent1["weights"][weight_id][:offspring_n_unit]
        parent1["weights"][weight_id] = temp
        temp = []
        temp = parent2["weights"][weight_id][:offspring_n_unit]
        parent2["weights"][weight_id] = temp
        # take smallest number of units
        next_min = min(parent1["units"][unit_id_next], parent2["units"][unit_id_next])
        if (np.array(parent1["weights"][weight_id]).shape[1] > next_min):
            temp = []
            for i in range(len(parent1["weights"][weight_id])):
                temp.append(parent1["weights"][weight_id][i][:next_min])
            parent1["weights"][weight_id] = temp
        if (np.array(parent2["weights"][weight_id]).shape[1] > next_min):
            temp = []
            for i in range(len(parent2["weights"][weight_id])):
                temp.append(parent2["weights"][weight_id][i][:next_min])
            parent2["weights"][weight_id] = temp
        # print(f"-------------After----------------")
        # print(np.array(parent1["weights"][weight_id]).shape)
        # print(np.array(parent2["weights"][weight_id]).shape)
        return parent1, parent2

    def activatioNCrossover(self, p1_activation, p2_activation, currLayer, maxLayers, offSpringNumber):
        if (currLayer < maxLayers / 2):
            if offSpringNumber == 1:
                return p1_activation[currLayer]
            else:
                return p2_activation[currLayer]
        else:
            if offSpringNumber == 1:
                return p2_activation[currLayer]
            else:
                return p1_activation[currLayer]

    def weightsCrossover(self, p1_weights, p2_weights, offSpringNumber):
        crossover_point = np.uint32(len(p1_weights) / 2)
        if (offSpringNumber == 1):
            temp1 = []
            temp2 = []
            temp = []
            temp1.append(p1_weights[crossover_point:])
            temp2.append(p2_weights[:crossover_point])
            temp.append(temp1)
            temp.append(temp2)
        elif (offSpringNumber == 2):
            temp1 = []
            temp2 = []
            temp = []
            temp1.append(p2_weights[crossover_point:])
            temp2.append(p1_weights[:crossover_point])
            temp.append(temp1)
            temp.append(temp2)
        return temp

    def crossover(self, parent1_idx, parent2_idx):

        parent1 = self.training_logs[parent1_idx]
        parent2 = self.training_logs[parent2_idx]
        if self.debug == True:
            print(
                f'-------------Creating Offspring for Parent {self.training_logs[parent1_idx]["parentID"]} and {self.training_logs[parent2_idx]["parentID"]}----------------')
            print(parent1["units"])
            print(parent1["activation"])
            print(parent2["units"])
            print(parent2["activation"])
        # [r]take smallest number of layers
        offspring_n_layers = min(len(parent1["units"]), len(parent2["units"]))
        offspring1 = Sequential()
        offspring2 = Sequential()
        id = len(self.activation_function_list)
        for i in range(offspring_n_layers - 1):
            # print(f"-------------Before----------------")
            # print("parent 1 layer_"+str(i)+"_weight")
            # print(parent1["weights"]["layer_"+str(i)+"_weight"])
            # print("parent 2  layer_"+str(i)+"_weight")
            # print(parent2["weights"]["layer_"+str(i)+"_weight"])
            activ_func1 = self.activatioNCrossover(parent1["activation"], parent2["activation"], i,
                                                   offspring_n_layers - 1, 1)
            activ_func2 = self.activatioNCrossover(parent1["activation"], parent2["activation"], i,
                                                   offspring_n_layers - 1, 2)
            if i == 0:
                self.activation_function_list[id] = ["relu"]
                self.activation_function_list[id + 1] = ["relu"]
            else:
                self.activation_function_list[id].append(activ_func1)
                self.activation_function_list[id + 1].append(activ_func2)
            parent1, parent2 = self.reorganise_parameters(parent1, parent2, i)
            offspring1.add(
                Dense(min(parent1["units"]["layer_" + str(i) + "_unit"], parent2["units"]["layer_" + str(i) + "_unit"]),
                      input_dim=self.input_size, activation=activ_func1))
            offspring2.add(
                Dense(min(parent1["units"]["layer_" + str(i) + "_unit"], parent2["units"]["layer_" + str(i) + "_unit"]),
                      input_dim=self.input_size, activation=activ_func2))

            # reorganise parameters for a layer
            # print(f"-------------After----------------")
            # print("parent 1 layer_"+str(i)+"_weight")
            # print(parent1["weights"]["layer_"+str(i)+"_weight"])
            # print("parent 2  layer_"+str(i)+"_weight")
            # print(parent2["weights"]["layer_"+str(i)+"_weight"])
            # self.training_logs[parent1_idx]["units"][0] =

        self.activation_function_list[id].append("sigmoid")
        self.activation_function_list[id + 1].append("sigmoid")
        offspring1.add(Dense(self.output_size, activation="sigmoid"))
        offspring2.add(Dense(self.output_size, activation="sigmoid"))
        # [r]for layer 0 crossover needs to added
        # i = 0
        # for i in range(1,offspring_n_layers - 1):
        # print("-------------")
        # x1 = self.weightsCrossover(parent1["weights"]["layer_"+str(i)+"_weight"],parent2["weights"]["layer_"+str(i)+"_weight"],1)
        # print(f"Curr unit {offspring1.layers[i].units} Next unit {offspring1.layers[i+1].units}")
        # print(np.array(x1).shape)
        # print(x1)
        # offspring1.layers[i].set_weights(x1)
        self.new_population.append(offspring1)
        self.new_population.append(offspring2)
        # print(self.activation_function_list)
        # print(f"-------------Offspring 1----------------")
        # print(offspring1.summary())
        # print(f"-------------Offspring 2----------------")
        # print(offspring1.summary())

        # offspring = numpy.empty(offspring_size)
        # # The point at which crossover takes place between two parents. Usually, it is at the center.
        # crossover_point = numpy.uint32(offspring_size[1]/2)
        # for k in range(offspring_size[0]):
        #     # Index of the first parent to mate.
        #     parent1_idx = k%parents.shape[0]
        #     # Index of the second parent to mate.
        #     parent2_idx = (k+1)%parents.shape[0]
        #     # The new offspring will have its first half of its genes taken from the first parent.
        #     offspring[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]
        #     # The new offspring will have its second half of its genes taken from the second parent.
        #     offspring[k, crossover_point:] = parents[parent2_idx, crossover_point:]
        # return offspring

    def select_mating_pool(self):
        # Selecting the best individuals in the current generation as parents for producing the offspring of the next generation.
        accuracy_dict = {}
        temp_training_logs = {}
        for log in range(len(self.training_logs)):
            accuracy_dict[log] = self.training_logs[log]["accuracy"]
        accuracy_dict = dict(sorted(accuracy_dict.items(), key=lambda item: item[1], reverse=True))
        i = 0
        for a in accuracy_dict.keys():
            temp_training_logs[i] = self.training_logs[a]
            i += 1
        return temp_training_logs

    def trainIndividual(self, individualId):
        log = {}

        print(f"-------------Training Parent {individualId + 1} / {len(self.population)}----------------")
        self.population[individualId].compile(optimizer='adam',
                                              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                                              metrics=['accuracy'])
        history = self.population[individualId].fit(train_images, train_labels, epochs=self.epochs_per_sol,
                                                    validation_data=(test_images, test_labels))
        log["accuracy"] = history.history["accuracy"][0]
        log["parentID"] = individualId + 1
        log["units"] = {}
        log["weights"] = {}
        # log["bias"] =  {}
        log["activation"] = self.activation_function_list[individualId]
        log["layers"] = self.population[individualId].layers  # Can maybe remove

        for layer in range(len(log["layers"])):
            log["weights"]["layer_" + str(layer) + "_weight"] = log["layers"][layer].weights[0].numpy()
            log["units"]["layer_" + str(layer) + "_unit"] = log["layers"][layer].units
            # log["bias"]["layer_"+str(layer)+"_bias"] = log["layers"][layer].bias.numpy()
        self.training_logs[len(self.training_logs)] = log
        self.save_progress.append({
            "accuracy": log["accuracy"],
            "parentID": log["parentID"],
            "architecture": self.population[individualId].to_json()
        })

        collection.update_one({
            "email": self.userEmailId
        }, {
            "$set": {
                "status": "processing",
                "state_uptil_now": self.save_progress
            }
        })
        # print(self.training_logs[len(self.training_logs) - 1])

    def result(self):
        best = self.select_mating_pool()[0]
        selected_model = self.population[best["parentID"] - 1]
        print("Best accuracy:")
        print(best["accuracy"])
        print("Best architecture:")
        print(selected_model.to_json())
        collection.update_one({
            "email": self.userEmailId
        }, {
            "$set": {
                "status": "done",
                "state_uptil_now": self.save_progress,
                "best_architecture": selected_model.to_json(),
                "best_accuracy": best["accuracy"]
            }
        })

    def train(self):
        collection.find_one_and_update({
            "email": self.userEmailId
        }, {
            "$set": {
                "email": self.userEmailId,
                "status": "processing"
            }
        }, upsert=True)

        for generation in range(self.num_generations):
            start_time = time.time()
            self.training_logs = {}
            print(f"Generation : {generation + 1} / {self.num_generations}")
            for individual in range(self.sol_per_pop):
                self.trainIndividual(individual)
            self.training_logs = self.select_mating_pool()
            self.activation_function_list = {}
            for i in range(int(self.sol_per_pop / 2)):
                if i % 2 == 0 and i != 1:
                    self.crossover(i, i + 1)
            i = 0
            while (len(self.new_population) < self.sol_per_pop):
                self.new_population.append(self.population[self.training_logs[i]["parentID"] - 1])
                self.activation_function_list[len(self.activation_function_list)] = self.training_logs[i]["activation"]
                i += 1
            self.population = self.new_population
            self.new_population = []
            print(f"Generation Training time:{(time.time() - start_time) / 60} minutes")
            # for log in range(len(self.training_logs)):
            #   print(self.training_logs[log])
        self.result()


# p = PopulationCreator()
# p.createModel()
# GeneticAlgorithm().train()
